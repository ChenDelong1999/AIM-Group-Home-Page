---
title: "VirtualConductor: Music-driven Conducting Video Generation System"
abstract: In this demo, we present the _VirtualConductor_, a system that can
  generate conducting video from a given piece of music and a single user's
  image. First, a large-scale conductor motion dataset is collected and
  constructed. Then, we propose an Audio Motion Correspondence Network (AMCNet)
  and adversarial-perceptual learning to learn the cross-modal relationship and
  generate diverse, plausible, music-synchronized motion. Finally, we combine 3D
  animation rendering and a pose transfer model to synthesize conducting video
  from a single given user's image. Therefore, any user can become a virtual
  conductor through the _VirtualConductor_ system.
slides: ""
url_pdf: publication/icme2021virtualconductor/VirtualConductor_Music_driven_Conducting_Video_Generation_System.pdf
publication_types:
  - "1"
authors:
  - 陈德龙
  - 刘凡
  - 李泽文
  - 许峰
publication: In *IEEE International Conference on Multimedia and Expo (ICME)
  2021, demo track*. [[ArXiv]](https://arxiv.org/abs/2108.04350)
featured: true
tags:
  - Music Information Retrieval
  - Self-supervised Learning
  - Deep Learning
  - Multimodal Learning
projects:
  - 音乐驱动的乐队指挥动作生成
summary: In this demo, we present the _VirtualConductor_, a system that can
  generate conducting video from a given piece of music and a single user's
  image. This demo won the ICME 2021 <font color=red>Best Demo</font> award.
url_dataset: ""
url_project: ""
publication_short: In *ICME 2021*
url_source: ""
url_video: https://www.bilibili.com/video/BV1aX4y1g7wh
date: 2021-07-05T00:00:00Z
url_slides: ""
links: null
image:
  preview_only: false
publishDate: 2021-07-05T00:00:00Z
url_poster: ""
url_code: ""
doi: ""
---

{{% callout note %}}
This demo won **ICME 2021 Best Demo** award.
{{% /callout %}}

